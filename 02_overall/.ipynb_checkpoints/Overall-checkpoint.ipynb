{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3255e27c-6084-4d1f-86bd-20316321c229",
   "metadata": {},
   "source": [
    "Day 1: General Introduction to various topics \n",
    "Day 2: Installtion of Python, Pycharm, Dataiku\n",
    "Day 3: Libries to be used, libraries installation and basic testing\n",
    "Day 4: CSV file reading using pandas for Machine Learning\n",
    "Day 5: Preprocessing\n",
    "Day 6:\n",
    "Day 7:\n",
    "Day 8: \n",
    "Day 9:\n",
    "Day 10:\n",
    "Day 11:\n",
    "Day 12:\n",
    "Day 13:\n",
    "Day 14:\n",
    "Day 15:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bcb9d-2741-4d67-b17b-476e7cb714cf",
   "metadata": {},
   "source": [
    "# Day 1: General Introduction to various topics "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1fa4af4-7925-4e8e-881e-0be1b4fc90d5",
   "metadata": {},
   "source": [
    "Data Science ---> It is the process of understanding the inbuilt pattern,\n",
    "analysis, forecasting, prediction of the data.\n",
    "\n",
    "Structured Data - MACHINE LEARNING\n",
    "\n",
    "CSV, EXCEL, DB(SQL, ORACLE),JSON\n",
    "\n",
    "Un-Structured Data - DEEP LEARNING\n",
    "\n",
    "IMAGES, AUDIOS, VIDEOS,\n",
    "\n",
    "Semi-Structured Data - NLP, LLM\n",
    "\n",
    "TEXT\n",
    "\n",
    "Data Science + Machine Learning\n",
    "\n",
    "Machine Learning: This is mechanism or algorithm in which it is going to help\n",
    "us in prediction and data analysis of structured data through some inbuilt libraries.\n",
    "\n",
    "Data Analytics:\n",
    "\n",
    "It is first level to the Data science. Finding and analysing the data and\n",
    "showcasing it in picture or visualization way to for the better understanding.\n",
    "\n",
    "Programming Language:\n",
    "\n",
    "Primary Focus:\n",
    "\n",
    "Python(Machine Learning) ---> DA + DS\n",
    "\n",
    "Secondary Focus:\n",
    "\n",
    "PowerBI or Tableau ---> DA (Visualization) (DS)\n",
    "GITHUB ----> Code Versioning (DA + DS)\n",
    "SQL or Oracle or Mongo ----> Storage (DA + DS)\n",
    "CSV or EXCEL ---> File Storage(DA + DS)\n",
    "\n",
    "Optional:\n",
    "MS Excel ---> DA\n",
    "\n",
    "=====================================================================================\n",
    "\n",
    "Python + Machine Learning\n",
    "\n",
    "Software Download: https://www.python.org/\n",
    "\n",
    "3.10 ,3.12, 3.13, 3.14\n",
    "\n",
    "SDK (Software Development KIT ) or Editor or IDE\n",
    "\n",
    "IDLE\n",
    "Pycharm-(Primary)\n",
    "Visual Studio\n",
    "Jupyter-(Secondary)\n",
    "\n",
    "Python Libraries:\n",
    "\n",
    "1. pandas (file processing, file manipulation, file preprocessing)\n",
    "2. numpy   (mathematical, stats)\n",
    "3. scikit-learn (machine learning)\n",
    "4. matplotlib (visualization)\n",
    "5. seaborn (visualization)\n",
    "\n",
    "6. scipy (stats)\n",
    "7. statsmodels(stats)\n",
    "\n",
    "8. pytorch (Deep Learning)\n",
    "9. tensorflow (Deep Learning)\n",
    "\n",
    "Machine Learning and AI can be achieved without code as well.\n",
    "\n",
    "NoCode DataScience\n",
    "Tool:\n",
    "\n",
    "Dataikq\n",
    "Cloud (Azure)\n",
    "CI/CD - Jenkins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bf575-b334-47d9-b698-4680513313d4",
   "metadata": {},
   "source": [
    "# Day 2: Installtion of Python, Pycharm, Dataiku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2499cf5-4e9a-4d23-a6e9-f43f7eae97ef",
   "metadata": {},
   "source": [
    "# Day 3: Libries to be used, libraries installation and basic testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15dc378-0411-439b-8c98-882bb000855f",
   "metadata": {},
   "source": [
    "List of libraries used. \n",
    "\n",
    "File reading and manipulation:\n",
    "pandas\n",
    "numpy\n",
    "\n",
    "For Machine learning\n",
    "scikit-learn\n",
    "\n",
    "For Visualization\n",
    "matplotlib\n",
    "seaborn\n",
    "\n",
    "For statistics\n",
    "scipy\n",
    "statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269faa5-4b99-456f-aac2-4770823316c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Programming Checking \n",
    "print(\"Welcome to my program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f21d01-2b01-4b98-b264-f48383a47beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#package checking \n",
    "import pandas\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "print(\"scikit-learn is successfully imported!\")\n",
    "print(\"Version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0ab6a-abaf-4bb8-a566-7e58b4824ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library Testing \n",
    "import sklearn\n",
    "print(\"scikit-learn is successfully imported!\")\n",
    "print(\"Version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf125c4-49d5-4ad6-ab58-4580ce2877f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library Testing \n",
    "import pandas\n",
    "data={\"team\":[\"csk\",\"rcb\",\"rr\"]}\n",
    "df=pandas.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef6aac-fd10-4496-ae04-faf6fcfb2a43",
   "metadata": {},
   "source": [
    "# Day 4: CSV file reading using pandas for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a221279-9eb0-4863-83c1-dea7d3f2b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  carat        cut color clarity  depth  table  price     x  \\\n",
      "0               1   0.23      Ideal     E     SI2   61.5   55.0    326  3.95   \n",
      "1               2   0.21    Premium     E     SI1   59.8   61.0    326  3.89   \n",
      "2               3   0.23       Good     E     VS1   56.9   65.0    327  4.05   \n",
      "3               4   0.29    Premium     I     VS2   62.4   58.0    334  4.20   \n",
      "4               5   0.31       Good     J     SI2   63.3   58.0    335  4.34   \n",
      "...           ...    ...        ...   ...     ...    ...    ...    ...   ...   \n",
      "53935       53936   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75   \n",
      "53936       53937   0.72       Good     D     SI1   63.1   55.0   2757  5.69   \n",
      "53937       53938   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66   \n",
      "53938       53939   0.86    Premium     H     SI2   61.0   58.0   2757  6.15   \n",
      "53939       53940   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83   \n",
      "\n",
      "          y     z  \n",
      "0      3.98  2.43  \n",
      "1      3.84  2.31  \n",
      "2      4.07  2.31  \n",
      "3      4.23  2.63  \n",
      "4      4.35  2.75  \n",
      "...     ...   ...  \n",
      "53935  5.76  3.50  \n",
      "53936  5.75  3.61  \n",
      "53937  5.68  3.56  \n",
      "53938  6.12  3.74  \n",
      "53939  5.87  3.64  \n",
      "\n",
      "[53940 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"diamonds.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d763a7e3-0b32-4b07-b0cb-210c010052c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'carat', 'cut', 'color', 'clarity', 'depth', 'table',\n",
      "       'price', 'x', 'y', 'z'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"below two lines prints the column names of the csv file\"\"\"\n",
    "column_names = data.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0adcd01-2a68-4805-8e96-513e2420cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0         carat         depth         table         price  \\\n",
      "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
      "mean   26970.500000      0.797940     61.749405     57.457184   3932.799722   \n",
      "std    15571.281097      0.474011      1.432621      2.234491   3989.439738   \n",
      "min        1.000000      0.200000     43.000000     43.000000    326.000000   \n",
      "25%    13485.750000      0.400000     61.000000     56.000000    950.000000   \n",
      "50%    26970.500000      0.700000     61.800000     57.000000   2401.000000   \n",
      "75%    40455.250000      1.040000     62.500000     59.000000   5324.250000   \n",
      "max    53940.000000      5.010000     79.000000     95.000000  18823.000000   \n",
      "\n",
      "                  x             y             z  \n",
      "count  53940.000000  53940.000000  53940.000000  \n",
      "mean       5.731157      5.734526      3.538734  \n",
      "std        1.121761      1.142135      0.705699  \n",
      "min        0.000000      0.000000      0.000000  \n",
      "25%        4.710000      4.720000      2.910000  \n",
      "50%        5.700000      5.710000      3.530000  \n",
      "75%        6.540000      6.540000      4.040000  \n",
      "max       10.740000     58.900000     31.800000  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"below two lines describes about the dataset\"\"\"\n",
    "data_description = data.describe()\n",
    "print(data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c8ad52-e6a5-451a-a347-0e3ecf0e6cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0      int64\n",
      "carat         float64\n",
      "cut            object\n",
      "color          object\n",
      "clarity        object\n",
      "depth         float64\n",
      "table         float64\n",
      "price           int64\n",
      "x             float64\n",
      "y             float64\n",
      "z             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"below codes shows the data types of each of the columns\"\"\"\n",
    "column_data_types =  data.dtypes\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357f0add-5d7b-4bbb-bc3f-3c0d6aed2145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53940 entries, 0 to 53939\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  53940 non-null  int64  \n",
      " 1   carat       53940 non-null  float64\n",
      " 2   cut         53940 non-null  object \n",
      " 3   color       53940 non-null  object \n",
      " 4   clarity     53940 non-null  object \n",
      " 5   depth       53940 non-null  float64\n",
      " 6   table       53940 non-null  float64\n",
      " 7   price       53940 non-null  int64  \n",
      " 8   x           53940 non-null  float64\n",
      " 9   y           53940 non-null  float64\n",
      " 10  z           53940 non-null  float64\n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 4.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_information = data.info()\n",
    "print(data_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3fd8e83-b3b0-4df4-9300-25f92f36cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Ideal\n",
      "1          Premium\n",
      "2             Good\n",
      "3          Premium\n",
      "4             Good\n",
      "           ...    \n",
      "53935        Ideal\n",
      "53936         Good\n",
      "53937    Very Good\n",
      "53938      Premium\n",
      "53939        Ideal\n",
      "Name: cut, Length: 53940, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"below code is to return the rows of the given one column\"\"\"\n",
    "data_column = data['cut']\n",
    "print(data_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ccfd0-158e-41d6-94ae-a54495245cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"below syntax is to return rows of more than one column\"\"\"\n",
    "data_column = data[['cut',\"carat\",\"x\"]]\n",
    "print(data_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4123dc-781c-4091-963d-865c9e5d424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows_top = data.head()\n",
    "print(data_rows_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0098c-01c3-4b01-99f8-85f5ee733f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows_bottom = data.tail()\n",
    "print(data_rows_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ed3d1-8d89-4b55-8c6a-9102c9ca48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows_top_specification = data.head(10)\n",
    "print(data_rows_top_specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bca735-74b7-4edc-92fc-b45dee7fc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows_down_specification = data.tail(10)\n",
    "print(data_rows_down_specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5803f3e3-2aa0-436a-ad76-32ff19dc76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"below syntax of retrieving rows with index number is called slicing\"\"\"\n",
    "data_from_range = data[0:10] #0th index to 9th index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563ff1d-579a-49ad-8166-96db37e3adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_range = data[:20] #0th to 19th index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8892441-72f8-47e9-8bef-e78705a3b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_range = data[10:] #10th to last index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd22241-ff39-4fdc-9d00-8c0e8869f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"below three is similar to slicing but has one difference which in the last element\"\"\"\n",
    "\n",
    "data_from_range = data.loc[0:10] #0th index to 10th index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d585b-b5ed-421e-9727-4700ebc57962",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_range = data.loc[:20] #0th to 20th index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e04d1c-6bf3-485f-9729-77415a243a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_range = data.loc[10:] #10th to last index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a0017-46da-476c-9282-201faffbafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"below three lines is to fetch rows with columns\"\"\"\n",
    "\n",
    "data_from_range = data.loc[0:10,\"carat\"] #0th index to 10th index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fb1db-cba6-4dbf-a205-9256c829e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_range = data.loc[:20,[\"carat\",\"cut\",\"x\"]] #0th to 20th index\n",
    "print(data_from_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23420d-1f67-4e67-8eff-17e4de8a4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_range = data.loc[10:] #10th to last index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df81b0-e7ed-42ae-bb35-9097601d4388",
   "metadata": {},
   "source": [
    "# Day 5 Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb046b-7de6-4e42-a990-915a5ae65740",
   "metadata": {},
   "source": [
    "My Notes\n",
    "\n",
    "paranthesis () means tuple\n",
    "Square bracket means \n",
    "\n",
    "Preprocessing of data\n",
    "\n",
    "Checking the numeric and non numeric columns \n",
    "Checking the null data\n",
    "\n",
    "\n",
    "When filling with pandas, we can fill both categorical and numerical data. But when we are filling with simpleimputer we can fill only numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab15d46-ecd1-4a56-8377-253e241be76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interview Question\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('diamonds.csv')\n",
    "print(df)\n",
    "\n",
    "column = df.columns\n",
    "print(column)\n",
    "\n",
    "numeric_column = []\n",
    "non_numeric_column = []\n",
    "\n",
    "for col in column:\n",
    "    if df[col].dtype == 'O':\n",
    "        non_numeric_column.append(col)\n",
    "    else:\n",
    "        numeric_column.append(col)\n",
    "\n",
    "print(numeric_column)\n",
    "print(non_numeric_column)\n",
    "\n",
    "\n",
    "# paranthesis () means tuple\n",
    "# Square bracket means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aaba05-d9a2-43a4-8ef0-b2ea06de14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Part 1\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# df=pd.read_csv('diamonds.csv')\n",
    "# print(df)\n",
    "\n",
    "# null_check = df.isnull()\n",
    "# print(null_check)\n",
    "\n",
    "# print('***********')\n",
    "\n",
    "# finding_null = df.isnull().sum()\n",
    "# print(finding_null)\n",
    "\n",
    "# print('*************')\n",
    "\n",
    "# total_null = df.isnull().sum().sum()\n",
    "# print(total_null)\n",
    "\n",
    "# print('*************')\n",
    "\n",
    "# df.dropna(inplace=True)\n",
    "\n",
    "# \"\"\"Below code is for filling the null value\"\"\"\n",
    "# null_check = df.isnull()\n",
    "# print(null_check)\n",
    "\n",
    "# print('***********')\n",
    "\n",
    "#below code is to fill the generic values\n",
    "#df.fillna(1000, inplace=True)\n",
    "\n",
    "\"\"\"below code is to fill specific value (forward filling- if a row exist earlier to this row that value will be filled in below row)\"\"\"\n",
    "#df.ffill(inplace=True)\n",
    "\n",
    "\"\"\"below code is to fill specific value (backward filling- if a row exist below to this row that value will be filled in above row)\"\"\"\n",
    "#df.bfill(inplace=True)\n",
    "\n",
    "\"\"\"Filling the empty cells with specific data on certain columns\"\"\"\n",
    "#df['carat'] = df['carat'].fillna(0.25)\n",
    "\n",
    "\"\"\"Filling the empty cells with specific data on multiple columns with calculation\"\"\"\n",
    "#df[['carat','cut']] = df[['carat','cut']].fillna(df['price'].mean())\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('diamonds.csv')\n",
    "print(df)\n",
    "\n",
    "simple_imputer_object = SimpleImputer()\n",
    "df['price'] = simple_imputer_object.fit_transform(df[['price']])\n",
    "\n",
    "print(df)\n",
    "\n",
    "print('*************')\n",
    "\n",
    "finding_null = df.isnull().sum()\n",
    "print(finding_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce3a9d-df22-4b74-bfc4-e2db372e656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Part 2\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('diamonds.csv')\n",
    "print(df)\n",
    "\n",
    "simple_imputer_object = SimpleImputer()\n",
    "df['price'] = simple_imputer_object.fit_transform(df[['price']])\n",
    "\n",
    "print(df)\n",
    "\n",
    "print('*************')\n",
    "\n",
    "finding_null = df.isnull().sum()\n",
    "print(finding_null)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b630e1-e169-47ae-aff8-52b9354f3e9f",
   "metadata": {},
   "source": [
    "# Day 6: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9f49c-b661-463c-99e2-310a780d0d2d",
   "metadata": {},
   "source": [
    "Preprocessing is the concept of keeping the dataset prepared for model. \n",
    "1. Null Values\n",
    "    1.1 Filling the null Values\n",
    "    1.2 Dropping the null values \n",
    "    Library: import pandas\n",
    "             from sklearn.impute import simpleImputer\n",
    "\n",
    "2. Duplicate Finidng\n",
    "    2.1 Duplicate Finidng \n",
    "    2.2 Duplicate Removal \n",
    "    Library: import pandas\n",
    "\n",
    "3. Data Preprocessing(Feature Engineering)\n",
    "    3.1 Category to Numerical conversion\n",
    "    Library: import pandas\n",
    "             from sklearn.preprocessing import LabelEncoder\n",
    "             from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "4. Data Preprocessing(Feature Scaling)\n",
    "   Note: One form of Numerical data into Another form of numerical data\n",
    "    4.1 Standard Scalar\n",
    "    Library: from sklearn.preprocessing import StandardScalar\n",
    "\n",
    "    4.2 MinMax Scalar   \n",
    "    Library: from sklearn.preprocessing import MinMaxScalar              \n",
    "\n",
    "5. Data Understanding\n",
    "    5.1 Input Data\n",
    "        Input data can also be called as feature data/ independent data or X Data\n",
    "    5.2 Output Data \n",
    "        Output data can also be called as dependent data or target data or Y data\n",
    "\n",
    "6. Visualization\n",
    "   It is the process of representing our input and output data in pictorial or graphical format. \n",
    "   Library: Import matplotlib\n",
    "            Import seaborn\n",
    "    6.1 Dimension\n",
    "        6.1.1 Single Dimension\n",
    "        6.1.2 Double Dimension (Scope limited to two dimension) \n",
    "            a. Univariant  (Only one column data)\n",
    "            b. Bivariant (Need two columns for the representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5ae25-36fd-4679-9311-42b321ff87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "print(df)\n",
    "\n",
    "# finding_category_count = df['cut'].value_counts()\n",
    "# print(finding_category_count)\n",
    "\n",
    "# finding_category_count_total = df['cut'].value_counts().sum()\n",
    "# print(finding_category_count_total)\n",
    "\n",
    "# unique_category_names = df['cut'].unique()\n",
    "# print(unique_category_names)\n",
    "\n",
    "# total_unique_category_names = df['cut'].nunique()\n",
    "# print(total_unique_category_names)\n",
    "\n",
    "# df['cut'] = df['cut'].map({\"Ideal\":0,\"Fair\":1,\"Good\":2,\"Very Good\":3,\"Premium\":4})\n",
    "# print(df)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(df['color'])\n",
    "print(\"********\")\n",
    "label_encoder_object = LabelEncoder()\n",
    "df['color'] = label_encoder_object.fit_transform(df['color'])\n",
    "print(df['color'])\n",
    "\n",
    "\n",
    "print(label_encoder_object.classes_)\n",
    "\n",
    "print(label_encoder_object.transform(label_encoder_object.classes_))\n",
    "\n",
    "class_label = label_encoder_object.classes_\n",
    "class_label_number = label_encoder_object.transform(label_encoder_object.classes_)\n",
    "testing_mapping = dict(zip(class_label, class_label_number))\n",
    "print(testing_mapping)\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# print(df['color'])\n",
    "# print(\"********\")\n",
    "# ordinal_encoder_object = OrdinalEncoder()\n",
    "# df['color'] = ordinal_encoder_object.fit_transform(df[['color']])\n",
    "# print(df['color'])\n",
    "\n",
    "\"\"\"Ordinal encoder will not transform a NaN/Empty value, but the label encoder will even consider the NaN/Empty values also\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3e1a8-575e-4c33-bda0-8d7f42715b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e8276-9b09-4e39-8a5b-132c053a3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_category_count = df['cut'].value_counts()\n",
    "print(finding_category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aceb76d-5c15-4c56-96f3-0cf66c3a9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_category_count_total = df['cut'].value_counts().sum()\n",
    "print(finding_category_count_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898261c-d281-44ea-83cb-94371676bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_category_names = df['cut'].unique()\n",
    "print(unique_category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337e534-d944-4e0e-bf17-d4f1e2fd1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_category_names = df['cut'].nunique()\n",
    "print(total_unique_category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168e284-8b48-45c1-a463-6e30fb0bceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cut'] = df['cut'].map({\"Ideal\":0,\"Fair\":1,\"Good\":2,\"Very Good\":3,\"Premium\":4})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b63b91-df27-47c3-89ea-e02f61608fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(df['color'])\n",
    "print(\"********\")\n",
    "label_encoder_object = LabelEncoder()\n",
    "df['color'] = label_encoder_object.fit_transform(df['color'])\n",
    "print(df['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe8ce5-4bca-413d-ba2d-fb9688aae378",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_encoder_object.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4444104-ac7e-4a2e-8df0-886536eee2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_encoder_object.transform(label_encoder_object.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d74a83-a599-4527-b309-35da41afdcd8",
   "metadata": {},
   "source": [
    "# Day 7: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde1149-990b-461e-995d-c3e957baa573",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization:\n",
    "Visualization with respect to data science and data analytics can be done in two ways \n",
    "1. Python Library\n",
    "    1.1 matplotlib (Basic chart)\n",
    "    1.2 seaborn (advanced)\n",
    "    Installation: pip install matplotlib\n",
    "                  pip install seaborn\n",
    "    Visualization chart can be of two types:\n",
    "    1. Univariant\n",
    "    2. Bivariant   \n",
    "General Classification:\n",
    "    1. Only Numerical\n",
    "    2. Numerical with categorical data\n",
    "    3. Numeric with Numeric data\n",
    "\n",
    "2. PowerBI\n",
    "PowerBI is a tool that was developed on top of microsoft excel. \n",
    "PowerBI is interactive Visualization tool\n",
    "So PowerBI can be used in two ways \n",
    "1. Through powerBI desktop \n",
    "2. Through powerBI service (Entreprise/ Licensed)\n",
    "Note: Cannot access in PowerBI desktop MAC\n",
    "\n",
    "\n",
    "EDA -  Exploratory Data Analysis\n",
    "used for finding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6727f7-1cad-4f4c-9011-8ee23f958920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x=[1,2,3,4,5]\n",
    "# y=[2,4,6,8,10]\n",
    "\n",
    "# plt.plot(x,y,marker='*')\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"*************************\"\"\"\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x=[1,2,5,4,5]\n",
    "# y=[2,4,3,8,9]\n",
    "\n",
    "# plt.scatter(x,y,color='red')\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"*************************\"\"\"\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x=[1,2,5,4,5]\n",
    "# y=[2,4,3,8,9]\n",
    "# plt.plot(x,y)\n",
    "# plt.scatter(x,y,c=x,cmap='rainbow')\n",
    "# plt.colorbar()\n",
    "# plt.xlabel(\"X-axis\")\n",
    "# plt.ylabel(\"Y-axis\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\"\"\"*************************\"\"\"\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x=[-900, 100, 200, 300,400, 500, 600, 700, 800, 4000]\n",
    "# plt.boxplot(x)\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"*************************\"\"\"\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# team=['csk','kkr','rcb','mi','dc']\n",
    "# trophy=[5,3,2,5,1]\n",
    "# plt.bar(team,trophy,color='orange')\n",
    "# plt.xlabel(\"Teams\")    \n",
    "# plt.ylabel(\"Trophies\")\n",
    "# plt.title(\"IPL Trophies\")  \n",
    "# plt.show()  \n",
    "\n",
    "\"\"\"*************************\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,axes=plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "team=['csk','kkr','rcb','mi','dc']\n",
    "trophy=[5,3,2,5,1]\n",
    "axes[0].bar(team,trophy,color='orange')\n",
    "axes[0].set_title(\"IPL Trophies\")\n",
    "\n",
    "\n",
    "x=[1,2,5,4,5]\n",
    "y=[2,4,3,8,9]\n",
    "axes[1].scatter(x,y,color='red')\n",
    "axes[1].set_title(\"Scatter Plot\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  \n",
    "\n",
    "\"\"\"*************************\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b3430-2a0d-467c-a5bb-1f124cfee12f",
   "metadata": {},
   "source": [
    "# Day 8: Distribution, Feature Scaling and Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5c66a-a9a9-4ab0-b8f1-e473f3371c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling - Normal Distribution\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "#Generate normal distribution data\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(loc=0, scale = 1, size = 1000) #(mean=0, std=1)\n",
    "\n",
    "#Plot normal distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(normal_data, kde=True, bins = 30, color='blue')\n",
    "plt.title(\"Normal Distribution (mean=0, std=1)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "print(normal_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239cb63-b9e3-49d2-a221-3768eb8fa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling - Uniform Distribution\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "#Generate uniform distribution data\n",
    "np.random.seed(42)\n",
    "uniform_data = np.random.uniform(low=-3, high = 3, size = 1000) #(min=-3, max=3)\n",
    "\n",
    "#Plot uniform distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(uniform_data, kde=True, bins = 30, color='blue')\n",
    "plt.title(\"Uniform Distribution (low=-3, max=3)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "print(uniform_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396d13e-651f-426a-99d7-6475d0bfe835",
   "metadata": {},
   "source": [
    "Featrure Scaling \n",
    "Process of converting Number from one form to another form \n",
    "\n",
    "Feature Scaling (Input Data):\n",
    "\n",
    "1. Standardization \n",
    "2. Normalization\n",
    "\n",
    "Library:\n",
    "\n",
    "1. from sklearn.preprocessing StandardScaler\n",
    "2. from sklearn.preprocessing MinMaxScaler\n",
    "\n",
    "When we should go for this feature scaling ?\n",
    "\n",
    "1. Whenever we use the model that working based on the distance based algorithm. \n",
    "2. Imagine we have dataset contains following columns \n",
    "    2.1 Price (Rupees - 30000,40000,50000)\n",
    "    2.2 No_of _Bedrooms (integer - 2,3,4)\n",
    "    2.3 City_rating (2,3,4,5)\n",
    "    \n",
    "\n",
    "\n",
    "Supervised:\n",
    "1. KNN (K-Nearest Neighbour)\n",
    "2. SVM(Support Vector machine)\n",
    "\n",
    "Unsupervised:\n",
    "1. K-Means clustering\n",
    "2. DBScan\n",
    "3. Hierarical Clustering \n",
    "\n",
    "StandardScaler:\n",
    "1. It can have negative value\n",
    "2. It produces normal distribution\n",
    "\n",
    "MinMaxScaler:\n",
    "1. It will show only positive value\n",
    "2. It produces uniform distribution\n",
    "\n",
    "Distribution:\n",
    "How data is spread across the sheet or column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a4316-0131-4eab-8677-79d73284d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "print(df)\n",
    "\n",
    "# standar_scaler_obj = StandardScaler()\n",
    "# df[\"price\"] = standar_scaler_obj.fit_transform(df[[\"price\"]])\n",
    "# print(df)\n",
    "\n",
    "\n",
    "min_max_scaler_obj = MinMaxScaler()\n",
    "df[\"price\"] = min_max_scaler_obj.fit_transform(df[[\"price\"]])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627724a3-8b12-441d-adfe-f4fc71608d07",
   "metadata": {},
   "source": [
    "Outlier_Detection\n",
    "Check the two Notebook files Removing Outlier using Percentile and Removing_Outlier_using_percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56de2b3-a414-40d6-9f7f-f36da54ee408",
   "metadata": {},
   "source": [
    "# Day 9 : Machine Learning Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225cb96b-28e5-4022-b13f-b890ca96a842",
   "metadata": {},
   "source": [
    "Linear Regression:\n",
    "1. based upon the variance between two columns we predict the outcome or prediction or forecast. \n",
    "\n",
    "2. Supervised algorithm\n",
    "\n",
    "3. It can work with single column and also with multiple column \n",
    "\n",
    "3.1 Columns are categorised as X and Y \n",
    "\n",
    "    3.1.1 x = data or feature data or independent data\n",
    "    3.1.2 y = data or target data or dependent data\n",
    "\n",
    "4. As a developer we will be deciding or by experience we will know what should be our x and what should be our y. \n",
    "\n",
    "5. Since the name tells \"Regression\" output or target will be \"Continuous\" data\n",
    "\n",
    "5.1 Input or X  or Feature Data can be continuous or regression. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fb41d-baef-4371-9b97-2b99b5b44e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = {\"year\": [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010],\n",
    "        \"House_Price\": [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "print(\"-------------------------\")\n",
    "X = df[[\"year\"]]\n",
    "y = df[\"House_Price\"]\n",
    "\n",
    "model_obj = LinearRegression()\n",
    "model_obj.fit(X, y)\n",
    "\n",
    "output_df = pd.DataFrame({\"year\":[2007,2011,2015]})\n",
    "print(output_df)\n",
    "print(\"-------------------------\")\n",
    "\n",
    "final_prediction = model_obj.predict(output_df)\n",
    "print(final_prediction)\n",
    "print(\"-------------------------\")\n",
    "\n",
    "output_df['predicted_price'] = final_prediction\n",
    "print(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093722d-263b-4571-9f8e-b7a07ab8ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"diamonds.csv\")\n",
    "print(df)\n",
    "\n",
    "finding_null = df.isnull().sum()\n",
    "print(finding_null)\n",
    "\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "column_names = df.columns\n",
    "print(column_names)\n",
    "print(\"-------------------------\")\n",
    "\n",
    "df['cut'] = df['cut'].map({\"Ideal\":1,\"Fair\":2,\"Good\":3,\"Very Good\":4,\"Premium\":5})\n",
    "print(df)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "\n",
    "\n",
    "df['clarity'] = df['clarity'].map({\"I1\":1,\"SI2\":2,\"SI1\":3,\"VS2\":4,\"VS1\":5,\"VVS2\":6,\"VVS1\":7,\"IF\":8})\n",
    "print(df)\n",
    "\n",
    "# label_encoder_object = LabelEncoder()\n",
    "# df['cut'] = label_encoder_object.fit_transform(df['cut'])\n",
    "# print(df['cut'])\n",
    "\n",
    "print(\"-------------------------\")\n",
    "\n",
    "x = df[[\"carat\",\"cut\",\"depth\",\"table\",\"x\",\"y\",\"z\"]]\n",
    "y= df[\"price\"]\n",
    "\n",
    "\n",
    "\n",
    "model_obj = LinearRegression()\n",
    "model_obj.fit(x,y)\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame({\"carat\":[0.3,0.2,0.33],\"cut\":[2,1,2],\"depth\":[61.5,59.8,62.1],\"table\":[55,57,54],\"x\":[3.5,3.0,3.2],\"y\":[3.5,3.0,3.2],\"z\":[2.5,2.0,2.2]})\n",
    "print(output_df)\n",
    "print(\"-------------------------\")\n",
    "\n",
    "final_prediction = model_obj.predict(output_df)\n",
    "print(final_prediction)\n",
    "print(\"-------------------------\")\n",
    "\n",
    "output_df['predicted_price'] = final_prediction\n",
    "print(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906378c-3c31-42ea-bd32-af9f68eeb86c",
   "metadata": {},
   "source": [
    "# Day 10 : Machine Learning Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c92f3b-1bad-4285-9dec-96fac607a02f",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Year  house_price\n",
    "2000    5000\n",
    "2001    6000\n",
    "2002    7000\n",
    "2003    8000\n",
    "2004    9000\n",
    "2005    10000\n",
    "2006    11000\n",
    "2007    12000\n",
    "2008    13000\n",
    "2009    14000\n",
    "\n",
    "total rows = 10\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7)\n",
    "train_size = 0.7 (70%) - 7 rows\n",
    "test_size = 0.3  (30%) - 3 rows\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "test_size = 0.3 (30%) - 3 rows\n",
    "train_size = 0.7 (70%) - 7 rows\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df[['Avg. Session Length','Time on App','Time on Website','Length of Membership']]\n",
    "y = df['Yearly Amount Spent']\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "-----------------------------------------------------------------------------------\n",
    "How train and test will happen: Random Sampling\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state = 42)\n",
    "\n",
    "Year  house_price\n",
    "=======================\n",
    "2000    5000   (Train)\n",
    "2001    6000   (Test)\n",
    "2002    7000   (Train)\n",
    "2003    8000   (Test)\n",
    "2004    9000   (Test)\n",
    "2005    10000  (Train)\n",
    "2006    11000  (Train)\n",
    "2007    12000  (Train)\n",
    "2008    13000  (Train)\n",
    "2009    14000  (Train)\n",
    "\n",
    "Random State:\n",
    "It is the random value to get generated with same set every time you run the program\n",
    "without closing.\n",
    "You can specify any number of you wish for it to work. (42 is not mandatory)\n",
    "\n",
    "Training and Testing Ratio:\n",
    "\n",
    "70:30(Training:Testing)\n",
    "80:20(Training:Testing)\n",
    "\n",
    "x_train   y_train         model.fit(x_train, y_train)\n",
    "2000    5000   (Train)\n",
    "2002    7000   (Train)\n",
    "2005    10000  (Train)\n",
    "2006    11000  (Train)\n",
    "2007    12000  (Train)\n",
    "2008    13000  (Train)\n",
    "2009    14000  (Train)\n",
    "\n",
    "x_test                   model.predict(x_test)\n",
    "2001       (Test)\n",
    "2003       (Test)\n",
    "2004       (Test)\n",
    "\n",
    "Prediction:\n",
    "\n",
    "[6000, 8000, 9000]\n",
    "===================================================================\n",
    "Why we do train and test?\n",
    "\n",
    "OverFitting: When model learns all the data about x and y. it will\n",
    "lead to overfitting.\n",
    "\n",
    "UnderFitting : When models trains with fewer set of inputs (x and y). it will lead\n",
    "to underfitting.\n",
    "================================================================================================\n",
    "\n",
    "Model Evaluation:\n",
    "\n",
    "We as user, when we predicted the output using x_test data, then we have to do the validation or\n",
    "evaluation.\n",
    "\n",
    "Evaluation (Metrics) is categorized into two types.\n",
    "\n",
    "1. Regression\n",
    "    1.1 r2_score (values between 0 to 1. anything above 0.75 is said to be good model)\n",
    "    1.2 mean_absolute_error\n",
    "    1.3 mean_squared_error\n",
    "\n",
    "2. Classification\n",
    "   2.1 accuracy_score\n",
    "   2.3 confusion_matrix\n",
    "   2.3 classification_report\n",
    "   2.4 f1_score\n",
    "   2.5 recall\n",
    "   2.6 precision\n",
    "\n",
    "=============================================================================================\n",
    "\n",
    "What happens if our r2_score shows below 0.5 for regression?\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Model is under performing for the given data.\n",
    "\n",
    "One of the Solution to better the accuracy:\n",
    "\n",
    "1. Try with larger rows of data\n",
    "(Note: Fewer input sometime may not produce greate accuracy)\n",
    "\n",
    "Scenario:\n",
    "Some time, trained data may be less efficient, but test data\n",
    "may have high efficient input.\n",
    "\n",
    "Resolution: Cross Validation.\n",
    "\n",
    "Cross Validation is the concept that ensures the overall dataset\n",
    "doesnt have partiality.\n",
    "\n",
    "cv = 5 (training and testing will happen with 5 different sets)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aeab11-e3bf-4ac1-8eb2-2136467b9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = pd.read_csv('Ecommerce_Customers.csv')\n",
    "print(df)\n",
    "\n",
    "\n",
    "sns.pairplot(df)\n",
    "plt.show()\n",
    "\n",
    "X = df[['Avg. Session Length','Time on App','Time on Website','Length of Membership']]\n",
    "\n",
    "y = df['Yearly Amount Spent']\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "final_prediction = model.predict(x_test)\n",
    "print(\"***************************\")\n",
    "print(final_prediction)\n",
    "print(\"***************************\")\n",
    "\n",
    "metric_evaluation = r2_score(y_test, final_prediction)\n",
    "print(\"R2 Score:\", metric_evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9291844c-f123-4cd9-8727-22f6efa4f2c3",
   "metadata": {},
   "source": [
    "# Day 11: Linear Regression with API Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27c477-a14c-416a-b435-88f3330153a5",
   "metadata": {},
   "source": [
    "API - Application Program Interface\n",
    "\n",
    "Python program can be converted into API through one of the following services\n",
    "\n",
    "1. Flask - Micro services\n",
    "2. Django - mvt\n",
    "    2.1 model (m)\n",
    "    2.2 view (v)\n",
    "    2.3 template (t)\n",
    "\n",
    "Installation \n",
    "pip install flask \n",
    "\n",
    "Flow for creation of an API:\n",
    "\n",
    "Step 1: Install the necessary library \n",
    "Step 2: Convert that program into application \n",
    "Step 3: Test by running an empty application\n",
    "    3.1 : IP Address\n",
    "    3.2 Port number\n",
    "\n",
    "Note: We need both IP address and Port number for creation of an application, since we are running it locally. \n",
    "IP addres and port number will be automatically picked by the program. \n",
    "\n",
    "\n",
    "DNS - \n",
    "FQDI - \n",
    "\n",
    "__name__ - double underscore is called as magic method\n",
    "\n",
    "\n",
    "API can be consumed with various methods\n",
    "1. through any programming language (python)\n",
    "2. through command\n",
    "3. through tools (postman or insomania)\n",
    "\n",
    "Note: Install postman "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fbb25-9927-4ab4-861a-b53c8bc86d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.py\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open(\"uptor_linear_trained_model.pkl\", \"rb\") as file_reading_obj:\n",
    "    train_model = pickle.load(file_reading_obj)\n",
    "\n",
    "my_app = Flask(__name__)\n",
    "\n",
    "\n",
    "@my_app.route('/linear_model_predict', methods=['POST'])\n",
    "def linear_predicttion():\n",
    "    data = request.get_json()\n",
    "    year_from_user = data.get(\"year\")\n",
    "    if not year_from_user or not isinstance(year_from_user, list):\n",
    "        return jsonify({\"error\": \"Invalid input, please validate the input.\"}), 400\n",
    "    data_input = pd.DataFrame({\"year\": year_from_user})\n",
    "    predicted_output = train_model.predict(data_input[[\"year\"]])\n",
    "    return jsonify({'data': predicted_output.tolist()})\n",
    "\n",
    "@my_app.route('/')\n",
    "def landing():\n",
    "    return \"Welcome to my Flask app!\"\n",
    "\n",
    "@my_app.route('/login')\n",
    "def login():\n",
    "    return \"This is the login page.\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    my_app.run(debug=True, port=5050)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5a189-2e1d-49a3-8fbb-53d6a1df7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_regression_API_calling.py\n",
    "import requests\n",
    "url = \"http://127.0.0.1:5050/linear_model_predict\"\n",
    "payload = {\"year\": [2007, 2011, 2015]}\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "print(response.text)\n",
    "print(response.json())\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84e291-c7e0-444a-9d69-2092bfc322ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear-regression_base_file.py\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = {\"year\": [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010],\n",
    "        \"House_Price\": [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "print(\"-------------------------\")\n",
    "X = df[[\"year\"]]\n",
    "y = df[\"House_Price\"]\n",
    "\n",
    "model_obj = LinearRegression()\n",
    "model_obj.fit(X, y)\n",
    "\n",
    "# output_df = pd.DataFrame({\"year\":[2007,2011,2015]})\n",
    "# print(output_df)\n",
    "# print(\"-------------------------\")\n",
    "\n",
    "# final_prediction = model_obj.predict(output_df)\n",
    "# print(final_prediction)\n",
    "# print(\"-------------------------\")\n",
    "\n",
    "# output_df['predicted_price'] = final_prediction\n",
    "# print(output_df)\n",
    "\n",
    "with open(\"uptor_linear_trained_model.pkl\", \"wb\") as file_obj:\n",
    "    pickle.dump(model_obj, file_obj)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b251b2b-b315-40b3-991a-4cf3528367af",
   "metadata": {},
   "source": [
    "# Day 12: Linear Regression with API Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8ed0-6069-4f84-bb28-098384f30587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.py\n",
    "from flask import Flask, jsonify, request\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open(\"diamonds_linear_trained_model.pkl\", \"rb\") as file_reading_obj:\n",
    "    train_model = pickle.load(file_reading_obj)\n",
    "\n",
    "diamond_app = Flask(__name__)   \n",
    "\n",
    "@diamond_app.route('/linear_model_predict', methods=['POST'])\n",
    "def linear_predicttion():\n",
    "    data = request.get_json()\n",
    "    \n",
    "    data_from_user = data.get(\"carat\",\"cut\",\"depth\",\"table\",\"x\",\"y\",\"z\")\n",
    "    if not data_from_user or not isinstance(data_from_user, list):\n",
    "        return jsonify({\"error\": \"Invalid input, please validate the input.\"}), 400\n",
    "    data_input = pd.DataFrame({\"carat\": data_from_user[0], \"cut\": data_from_user[1], \"depth\": data_from_user[2], \"table\": data_from_user[3], \"x\": data_from_user[4], \"y\": data_from_user[5], \"z\": data_from_user[6]})\n",
    "    predicted_output = train_model.predict(data_input)\n",
    "    return jsonify({'data': predicted_output.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    diamond_app.run(debug=True, port=5060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7676c6-2e43-4cc4-a60b-19d8d6ff0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_regression_base_file.py\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"diamonds.csv\")\n",
    "\n",
    "\n",
    "finding_null = df.isnull().sum()\n",
    "print(finding_null)\n",
    "\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "df['cut'] = df['cut'].map({\"Ideal\":1,\"Fair\":2,\"Good\":3,\"Very Good\":4,\"Premium\":5})\n",
    "\n",
    "df['clarity'] = df['clarity'].map({\"I1\":1,\"SI2\":2,\"SI1\":3,\"VS2\":4,\"VS1\":5,\"VVS2\":6,\"VVS1\":7,\"IF\":8})\n",
    "\n",
    "# label_encoder_object = LabelEncoder()\n",
    "# df['cut'] = label_encoder_object.fit_transform(df['cut'])\n",
    "# print(df['cut'])\n",
    "\n",
    "x = df[[\"carat\",\"cut\",\"depth\",\"table\",\"x\",\"y\",\"z\"]]\n",
    "y= df[\"price\"]\n",
    "\n",
    "model_obj = LinearRegression()\n",
    "model_obj.fit(x,y)\n",
    "\n",
    "\n",
    "with open(\"diamonds_linear_trained_model.pkl\", \"wb\") as file_obj:\n",
    "    pickle.dump(model_obj, file_obj)\n",
    "\n",
    "# output_df = pd.DataFrame({\"carat\":[0.3,0.2,0.33],\"cut\":[2,1,2],\"depth\":[61.5,59.8,62.1],\"table\":[55,57,54],\"x\":[3.5,3.0,3.2],\"y\":[3.5,3.0,3.2],\"z\":[2.5,2.0,2.2]})\n",
    "# print(output_df)\n",
    "# print(\"-------------------------\")\n",
    "\n",
    "# final_prediction = model_obj.predict(output_df)\n",
    "# print(final_prediction)\n",
    "# print(\"-------------------------\")\n",
    "\n",
    "# output_df['predicted_price'] = final_prediction\n",
    "# print(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9abd1-3065-408a-9446-1d39162d3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lineear_regression_API_calling.py\n",
    "import requests\n",
    "url = \"http://127.0.0.1:5060/linear_model_predict\"\n",
    "payload = {\"carat\": [0.5, 0.7, 1.0], \"cut\": [\"Ideal\", \"Premium\", \"Good\"], \"depth\": [60, 62, 65], \"table\": [55, 57, 58], \"x\": [3.5, 4.0, 4.5], \"y\": [4.0, 4.5, 5.0], \"z\": [2.5, 3.0, 3.5]}\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "print(response.text)\n",
    "print(response.json())\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048612a6-7bec-4e68-b68a-c90ab49aa9e1",
   "metadata": {},
   "source": [
    "# Day 14: Machine Learning Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f886d-e7f2-4627-8987-464c9a786eff",
   "metadata": {},
   "source": [
    "# Day 15: Machine Learning Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cdf40-bb20-404b-87b1-57c7a1f7d5a2",
   "metadata": {},
   "source": [
    "# Day 16: One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337b48c-4dcb-400f-ac72-8931ab48b5c9",
   "metadata": {},
   "source": [
    "# Day 17 : General Discussion with Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b103ea-e3ff-485d-9fff-0a4a02d08c20",
   "metadata": {},
   "source": [
    "# Day 18: Cross Validation, Stratfield, K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aac1e-5c35-41da-9bbe-1758fa0ee727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
