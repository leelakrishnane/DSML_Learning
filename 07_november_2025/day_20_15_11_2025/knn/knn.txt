KNN - Supervised Learning Algorithm

It can be used for

1. Regressor and classifier

KNN is distance based algorithm.

Distance internally is calculated based on

1. Manhatten
2. Eucliden


Since this is distance based algorithm we have to
do "standarization".

1. MinMaxScaler 2. StandardScaler

Notes:

## Notes

- k-NN is a simple yet powerful algorithm for classification and regression tasks.

- One of the main advantages of k-NN is that it is a non-parametric algorithm,
- which means that it does not make any assumptions about the distribution of the data.
-
- This makes it very versatile and applicable to a wide range of problems.

- However, k-NN can be computationally expensive,
- especially when dealing with large datasets or high-dimensional data.
-
- In addition, the choice of the distance metric and the value of k can have a significant impact on the performance of the algorithm,
- and may need to be carefully tuned.

- The curse of dimensionality is an important concept to keep in mind when working with k-NN.
- As the number of dimensions in the data increases, the amount of data required to maintain a certain level of accuracy increases exponentially.
- This can make it difficult to apply k-NN to high-dimensional data.

- Scaling and normalization of the data can also be important when working with k-NN,
- as it can help to ensure that all features are given equal weight in the distance metric calculation.




